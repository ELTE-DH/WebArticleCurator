#!/usr/bin/env python3
# -*- coding: utf-8, vim: expandtab:ts=4 -*-

"""Here comes the stuff to extract more URL or the text from a specific downloaded webpage"""

import sys

from newspaper import Article
from articleDateExtractor import extractArticlePublishedDate


def extract_article_urls_from_page(article_list_raw_html, settings):
    """
        extracts and returns as a list the URLs belonging to articles from an HTML code
    """
    urls = set()
    for code_line in settings['ARTICLE_LINK_FORMAT_RE'].findall(article_list_raw_html):
        code_line = settings['BEFORE_ARTICLE_URL_RE'].sub('', code_line)
        code_line = settings['AFTER_ARTICLE_URL_RE'].sub('', code_line)
        urls.add(code_line)
    return urls


def articles_to_corpus(article_list_only_urls, converter, file_out, settings, downloader, logger_):
    """
        converts the raw HTML code of an article to corpus format and saves it to the output file
    """
    for url in article_list_only_urls:
        # url_match = settings['URL_PATTERN'].match(url)
        # url_path = url_match.group(5)
        # print(url_path)
        if logger_.is_url_processed(url):
            print(url + ' has already been processed')
            continue
        # handling of time filtering when archive page URLs are not generated by date
        if settings['ARTICLE_LIST_URLS_BY_ID'] and not settings['ARTICLE_LIST_URLS_BY_DATE']:
            date_before_interval = False
            date_after_interval = False
            if settings['DATE_INTERVAL_USED']:
                article_date = extractArticlePublishedDate(url)
                date_before_interval = settings['DATE_FROM'] > article_date.date()
                date_after_interval = article_date.date() > settings['DATE_UNTIL']

            if not (date_before_interval or date_after_interval):
                article_raw_html = downloader.download_url(url)  # TODO: Either way we end up download_page(...)

            elif date_before_interval:
                print(date_before_interval)
                # return None # does not work LOL WTF
                sys.exit()
            else:
                continue
        else:
            article_raw_html = downloader.download_url(url)

        try:
            corpus_formatted_article = converter.convert_doc_by_json(article_raw_html, url)
        except ValueError as e:
            logger_.log(url, e)
            continue
        print(settings['article_begin_flag'], corpus_formatted_article, settings['article_end_flag'], sep='', end='',
              file=file_out)
        logger_.log(url, 'download OK')


# TODO: something like this...
def articles_to_corpus_newspaper(url, page_str, file_out, downloader, logger_):
    article = Article(url, memoize_articles=False)
    article.download(input_html=page_str)
    article.parse()
    print(article.title)
    print(article.publish_date)
    print(article.authors)
    print(article.text)
