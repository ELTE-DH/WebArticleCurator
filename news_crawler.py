#!/usr/bin/env python3
# -*- coding: utf-8, vim: expandtab:ts=4 -*-

# import external libs
import sys

from articleDateExtractor import extractArticlePublishedDate

# import own modules
from logger import Logger
from corpus_converter import CorpusConverter
from input_constants_wrapper import wrap_input_consants
from enhanced_downloader import WarcDownloader, WarcReader
from extractor import article_to_corpus
from news_archive_crawler import NewsArchiveCrawler


class NewsArticleCrawler:
    """
        1) Get the list of articles (eg. NewsArchiveCrawler)
        2) Download article pages
        3) Extract the text of articles from raw HTML
        4) save them in corpus format
    """
    def __init__(self, atricles_filename, archive_filename, download, settings):
        self._settings = settings
        self._logger_ = Logger(self._settings['log_file_articles'])

        self._file_out = open(self._settings['output_file'], 'a+', encoding=self._settings['encoding'])
        self._converter = CorpusConverter(self._settings['site_schemas'], self._settings['tags'])

        # Create new archive while downloading, or simulate download and read the archive
        if download:
            self._downloader = WarcDownloader(atricles_filename, self._logger_)
        else:
            self._downloader = WarcReader(atricles_filename, self._logger_)

        self._archive_downloader = NewsArchiveCrawler(self._settings, download, archive_filename)

    def _get_archive_urls(self):
        return self._archive_downloader.url_iterator()

    def _process_urls(self, it):
        for url in it:
            # "Download" article, extract text
            # Extract links to other articles...
            # Check for already extracted urls!
            
            # Handling of time filtering when archive page URLs are not generated by date
            if self._settings['ARTICLE_LIST_URLS_BY_ID'] and not self._settings['ARTICLE_LIST_URLS_BY_DATE']:
                date_before_interval = False
                date_after_interval = False
                if self._settings['DATE_INTERVAL_USED']:
                    article_date = extractArticlePublishedDate(url)
                    date_before_interval = self._settings['DATE_FROM'] > article_date.date()
                    date_after_interval = article_date.date() > self._settings['DATE_UNTIL']

                if not (date_before_interval or date_after_interval):
                    # TODO: Either way we end up download_page(...)
                    article_raw_html = self._downloader.download_url(url)

                elif date_before_interval:
                    print(date_before_interval)
                    # return None # does not work LOL WTF
                    sys.exit()
                else:
                    continue
            else:
                article_raw_html = self._downloader.download_url(url)
            article_to_corpus(url, article_raw_html, self._converter, self._file_out, self._settings, self._logger_)

    def download_and_extract_all_articles(self):
        self._process_urls(self._get_archive_urls())

    def __del__(self):
        self._file_out.close()


# run the whole thing
if __name__ == '__main__':
    # get the command line argument it is the configuration file's name
    try:
        current_task_config_filename = sys.argv[1]
    except IndexError:
        raise IndexError('Not enough or too many input arguments!\n'
                         'The program should be called like:\n'
                         'python web_crawler.py current_task_config.json')

    # read input data from the given files, initialize variables
    portal_settings = wrap_input_consants(current_task_config_filename)
    m = NewsArchiveCrawler(portal_settings, True, 'example-archive.warc.gz')
    m.url_iterator()  # Get the list of urls in the archive...
